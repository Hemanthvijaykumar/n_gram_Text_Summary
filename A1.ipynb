{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Text Summarization by Sentence Ranking using 1, 2 and 3-grams weights\n",
    "\n",
    "We will do 3 sentence rankings using 1-gram, 2-gram and 3-gram weights respectively. And combine them for an overall ranking of sentences of a text corpus.\n",
    "\n",
    "First start with necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# BeautifulSoup is used to remove html tags from the text\n",
    "from bs4 import BeautifulSoup \n",
    "import re # For regular expressions\n",
    "import csv\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time\n",
    "startTime = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open a Text file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Text File name: \n",
      "Cambridge Analytica whistleblower Christopher Wylie\n"
     ]
    }
   ],
   "source": [
    "# This will read the file from current working directory\n",
    "name = input(\"Enter Text File name: \\n\")\n",
    "filename = \"./summary/\" + name + \".txt\"\n",
    "text = open(filename,\"r\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Raw Sentences and get** *text* **into all lower case for processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = sent_tokenize(text)\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "**It is a 5 step process**\n",
    "\n",
    "1. n-gram Extraction from *text*\n",
    "- n-gram frequency distribution actoss the entire text - also removing ones with leading and/or trailing stop words\n",
    "- Writing all n-grams and its frequency as a dictionary and a csv for later use\n",
    "- Weighting of all sentences of the text using n-gram frequency\n",
    "- Writing out the sentence and its weight in a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 1-gram based Sentence Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. 1-gram Extraction from the *text*\n",
    "\n",
    "First let's see how sentences are ranked using 1-gram weights. Here the stopwords are removed for the purpose of summing over 1-grams in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all 1-grams in text\n",
    "one_grams = []\n",
    "# 1. Removing html tags\n",
    "text = BeautifulSoup(text, \"lxml\").get_text()\n",
    "# 2. Tokenize Sentences\n",
    "sentences = sent_tokenize(text)\n",
    "for i in range(len(sentences)):\n",
    "    # 3. Removing non-letter from i-th sentence\n",
    "    sentence = re.sub(\"[^a-zA-Z]\",\" \",sentences[i])\n",
    "    # 4. Tokenize words of i-th sentence of text\n",
    "    words = word_tokenize(sentence)\n",
    "    # 5. Remove stopwords from i-th sentence of text\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    for w in words:\n",
    "        if w == '':\n",
    "            continue\n",
    "        one_grams.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. 1-gram frequency distribution actoss the entire text - *stop words not included*\n",
    "\n",
    "Next compute frequency distribution of all unique 1-grams using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding 1-grams and its frequency distribution\n",
    "one_grams = nltk.FreqDist(one_grams)\n",
    "# Preparing to write as term-frequecy file\n",
    "Distr1 = []\n",
    "UniqOneGrams = list(one_grams.keys())\n",
    "WordVals = list(one_grams.values())\n",
    "Distr1.extend(UniqOneGrams)\n",
    "Distr1.extend(WordVals)\n",
    "L = int(len(Distr1)/2)\n",
    "Distr1 = np.array([Distr1], dtype=object)\n",
    "Distr1.shape = (2,L)\n",
    "Distr1 = Distr1.transpose()\n",
    "Distr1 = sorted(Distr1, key=lambda a_entry: a_entry[1], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Writing all 1-grams and its frequency as a dictionary and a csv for later use\n",
    "\n",
    "Create a dictionary of 1-grams and frequency for later use and write all in a csv file for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write doc, term, frequency as a Python Dictionary\n",
    "dic1 = {Distr1[i][0]: Distr1[i][1] for i in range(len(Distr1))}\n",
    "# Write doc, term, frequency as a csv file\n",
    "with open(\"1-grams of test with stopwords.csv\", \"w\", newline = '') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(Distr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Weighting of all sentences of the text using 1-gram frequency\n",
    "\n",
    "Using the 1-gram - frequency dictionary compute weight of sentence by adding frequency of all its 1-grams other than stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1gram = []\n",
    "for i in range(len(sentences)):\n",
    "    # 1. Removing non-letter from i-th sentence\n",
    "    sentence = re.sub(\"[^a-zA-Z]\",\" \",sentences[i])\n",
    "    # 2. Tokenize words of i-th sentence of text\n",
    "    words = word_tokenize(sentence)\n",
    "    # 3. Remove stopwords from i-th sentence of text\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # 4. Make list of i-th sentence and its weight in terms of 1-gram frequency\n",
    "    v = sum([dic1.get(w) for w in words])\n",
    "    sentence_weight = [sentence, v]\n",
    "    sentence_1gram.append(sentence_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Writing out the sentence and its weight in a csv file\n",
    "\n",
    "This is for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Sentence and its 1-gram weight as a csv file\n",
    "with open(\"Sentence weight based on 1-grams.csv\", \"w\", newline = '') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(sentence_1gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 2-gram based Sentence Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. 2-gram Extraction from the *text*\n",
    "\n",
    "Next let's see how sentences are ranked differently using 2-gram weights. Here the 2-grams with leading or trailing stopwords are removed for the purpose of summing over 2-grams in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all 2-grams\n",
    "bigrams = []\n",
    "for i in range(len(sentences)):\n",
    "    # 1. Removing non-letter from i-th sentence\n",
    "    sentence = re.sub(\"[^a-zA-Z]\",\" \",sentences[i])\n",
    "    # 2. Tokenize words of i-th sentence of text\n",
    "    words = word_tokenize(sentence)\n",
    "    bigrm = nltk.bigrams(words)\n",
    "    bigrm = list(bigrm)\n",
    "    # 3. Extend the list of bigrams across all sentences of text\n",
    "    bigrams.extend(bigrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.a 2-gram frequency distribution actoss the entire text - *also removing ones with leading and/or trailing stop words*\n",
    "\n",
    "Next compute frequency distribution of all unique 2-grams using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding bigrams and its frequency distribution\n",
    "bi_grams = nltk.FreqDist(bigrams)\n",
    "# Preparing to write as term-frequecy file\n",
    "Distr2 = []\n",
    "UniqBiGrams = list(bi_grams.keys())\n",
    "WordVals = list(bi_grams.values())\n",
    "Distr2.extend(UniqBiGrams)\n",
    "Distr2.extend(WordVals)\n",
    "L = int(len(Distr2)/2)\n",
    "Distr2 = np.array([Distr2])\n",
    "Distr2.shape = (2,L)\n",
    "Distr2 = Distr2.transpose()\n",
    "Distr2 = sorted(Distr2, key=lambda a_entry: a_entry[1], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.b 2-gram frequency distribution actoss the entire text - *also removing ones with leading and/or trailing stop words*\n",
    "\n",
    "In this step remove 2-grams with leading or trailing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [line for line in Distr2 if not line[0][0] in stop_words]\n",
    "b = [line for line in a if not line[0][1] in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Writing all 2-grams and its frequency as a dictionary and a csv for later use\n",
    "\n",
    "Create a dictionary of 2-grams and frequency for later use and write all in a csv file for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write doc, term, frequency as a Python Dictionary\n",
    "dic2 = {b[i][0]: b[i][1] for i in range(len(b))}\n",
    "# Write doc, term, frequency as a csv file\n",
    "with open(\"2-grams of test with stopwords removed.csv\", \"w\", newline = '') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Weighting of all sentences of the text using 2-gram frequency\n",
    "\n",
    "Using the 2-gram - frequency dictionary compute weight of sentence by adding frequency of all its 2-grams that do not contain any stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_2gram = []\n",
    "for i in range(len(sentences)):\n",
    "    # 1. Removing non-letter from i-th sentence\n",
    "    sentence = re.sub(\"[^a-zA-Z]\",\" \",sentences[i])\n",
    "    # 2. Tokenize words of i-th sentence of text\n",
    "    words = word_tokenize(sentence)\n",
    "    bigrm = nltk.bigrams(words)\n",
    "    bigrm = list(bigrm)\n",
    "    # 3. Make list of i-th sentence and its weight in terms of 2-gram frequency\n",
    "    v = sum([dic2.get(w) for w in bigrm if not dic2.get(w) is None])\n",
    "    sentence_weight = [sentence, v]\n",
    "    sentence_2gram.append(sentence_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Writing out the sentence and its weight based on 2-grams in a csv file\n",
    "\n",
    "This is for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Sentence and its 2-gram weight as a csv file\n",
    "with open(\"Sentence weight based on 2-grams.csv\", \"w\", newline = '') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(sentence_2gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 3-gram based Sentence Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. 3-gram Extraction from the *text*\n",
    "\n",
    "Next let's see how sentences are ranked differently using 3-gram weights. Here the 3-grams with leading or trailing stopwords are removed for the purpose of summing over 3-grams in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all 3-grams\n",
    "trigrams = []\n",
    "for i in range(len(sentences)):\n",
    "    # 3. Removing non-letter from i-th sentence\n",
    "    sentence = re.sub(\"[^a-zA-Z]\",\" \",sentences[i])\n",
    "    # 4. Tokenize words of i-th sentence of text\n",
    "    words = word_tokenize(sentence)\n",
    "    trigrm = nltk.trigrams(words)\n",
    "    trigrm = list(trigrm)\n",
    "    # 5. Extend the list of bigrams across all sentences of text\n",
    "    trigrams.extend(trigrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.a 3-gram frequency distribution actoss the entire text - *also removing ones with leading and/or trailing stop words*\n",
    "\n",
    "Next compute frequency distribution of all unique 3-grams using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding trigrams and its frequency distribution\n",
    "tri_grams = nltk.FreqDist(trigrams)\n",
    "# Preparing to write as term-frequecy file\n",
    "Distr3 = []\n",
    "UniqTriGrams = list(tri_grams.keys())\n",
    "WordVals = list(tri_grams.values())\n",
    "Distr3.extend(UniqTriGrams)\n",
    "Distr3.extend(WordVals)\n",
    "L = int(len(Distr3)/2)\n",
    "Distr3 = np.array([Distr3])\n",
    "Distr3.shape = (2,L)\n",
    "Distr3 = Distr3.transpose()\n",
    "Distr3 = sorted(Distr3, key=lambda a_entry: a_entry[1], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.b 3-gram frequency distribution actoss the entire text - *also removing ones with leading and/or trailing stop words*\n",
    "\n",
    "In this step remove 3-grams with leading or trailing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [line for line in Distr3 if not line[0][0] in stop_words]\n",
    "d = [line for line in c if not line[0][2] in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Writing all 3-grams and its frequency as a dictionary and a csv for later use\n",
    "\n",
    "Create a dictionary of 3-grams and frequency for later use and write all in a csv file for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write doc, term, frequency as a Python Dictionary\n",
    "dic3 = {d[i][0]: d[i][1] for i in range(len(d))}\n",
    "# Write doc, term, frequency as a csv file\n",
    "with open(\"3-grams of test with stopwords removed.csv\", \"w\", newline = '') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Weighting of all sentences of the text using 3-gram frequency\n",
    "\n",
    "Using the 3-gram - frequency dictionary compute weight of sentence by adding frequency of all its 3-grams that do not contain leading or trailing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_3gram = []\n",
    "for i in range(len(sentences)):\n",
    "    # 1. Removing non-letter from i-th sentence\n",
    "    sentence = re.sub(\"[^a-zA-Z]\",\" \",sentences[i])\n",
    "    # 2. Tokenize words of i-th sentence of text\n",
    "    words = word_tokenize(sentence)\n",
    "    trigrm = nltk.trigrams(words)\n",
    "    trigrm = list(trigrm)\n",
    "    # 3. Make list of i-th sentence and its weight in terms of 3-gram frequency\n",
    "    v = sum([dic3.get(w) for w in trigrm if not dic3.get(w) is None])\n",
    "    sentence_weight = [sentence, v]\n",
    "    sentence_3gram.append(sentence_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Writing out the sentence and its weight based on 3-grams in a csv file\n",
    "\n",
    "This is for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Sentence and its 3-gram weight as a csv file\n",
    "with open(\"Sentence weight based on 3-grams.csv\", \"w\", newline = '') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(sentence_3gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now combine 1, 2, 3-grams list and their frequency in the *text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distr = Distr1 + b + d\n",
    "with open(\"All-grams of test with stopwords.csv\", \"w\", newline = '') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(Distr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And compute Sentence Ranking using combined 1, 2, 3-grams weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_weight = []\n",
    "for i in range(len(sentences)):\n",
    "    # 1. Removing non-letter from i-th sentence\n",
    "    sentence = re.sub(\"[^a-zA-Z]\",\" \",sentences[i])\n",
    "    weight = sentence_1gram[i][1] + sentence_2gram[i][1] + sentence_3gram[i][1]\n",
    "    sentenceANDweight = [raw_sentences[i], weight]\n",
    "    sentence_weight.append(sentenceANDweight)\n",
    "sentence_weight = sorted(sentence_weight, key=lambda a_entry: a_entry[1], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Sentence Ranking using combined 1, 2, 3-grams weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Sentence and its all-gram weight as a csv file\n",
    "with open(\"Sentence weight based on all-grams.csv\", \"w\", newline = '') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(sentence_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:05.323112\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now() - startTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
